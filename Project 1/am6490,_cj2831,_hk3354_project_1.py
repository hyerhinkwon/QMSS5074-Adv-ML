# -*- coding: utf-8 -*-
"""am6490, cj2831, hk3354 - Project_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z18ZWDuvbX6BQvo7zVbOKfvwcqF_ZzpJ

# Your Uni : am6490, cj2831, hk3354
# Your Full name : Arsh Misra, Conor Jones, Flora Kwon
# Link to your Public Github repository with Final report:
https://github.com/hyerhinkwon/QMSS5074-Adv-ML.git

### Submission Due Date: 03/07/2025

# World Happiness Classification Competition
Goals :
- Understand how the models function
- Understand what the parameters control
- Learn from the model experimentation process
- Make a good looking notebook report
- Upload as a personal project on Github

**Overall Steps:**
1. Load datasets and merge them.
2. Preprocess data using Sklearn Column Transformer/ Write and Save Preprocessor function
3. Fit model on preprocessed data and save preprocessor function and model
4. Generate predictions from X_test data and submit predictions

## 0. Loading Datasets

Loading the World Happiness 2023 datasets
"""

# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

#Load the dataset
whr_df = pd.read_csv('https://raw.githubusercontent.com/hyerhinkwon/QMSS5074-Adv-ML/refs/heads/main/Project%201/WHR_2023.csv')

# Inspect the first few rows to understand the structure
whr_df

# Convert the regression target ('happiness_score') into classification labels
# We'll use quartiles to create 5 happiness categories: Very Low, Low, Average, High, Very High

# Define quartiles
whr_df['happiness_category'] = pd.qcut(whr_df['happiness_score'],
                                       q=5,
                                       labels=['Very Low', 'Low','Average', 'High', 'Very High'])

# Select features and target
X = whr_df.drop(columns=['happiness_score', 'happiness_category'])
y = whr_df['happiness_category']

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Convert y_train and y_test to numerical labels
y_train_labels = y_train.astype('category').cat.codes
# y_test_labels = ## Complete in a similar manner as above
y_test_labels = y_test.astype('category').cat.codes

X_train = X_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)
y_train_labels = y_train.reset_index(drop=True)
y_test_labels = y_test.reset_index(drop=True)

"""Write in the next cell what the y_train.astype('category').cat.codes line does. What is the difference between y_train_labels and y_train?

**Your answer:** y_train.astype('category').cat.codes assigns numeric codes to categorical variables. y_train contains the original categorical variables in its original format and y_train_labels contains the numeric codes corresponding to each category in y_train.

<h3> Add new data
"""

# Truncated and cleaned up region data to merge
countrydata=pd.read_csv('https://raw.githubusercontent.com/hyerhinkwon/QMSS5074-Adv-ML/refs/heads/main/Project%201/newcountryvars.csv')

countrydata.head()

# Merge in new data to X_train and X_test by taking "country" from first table and "country_name" from 2nd table.
# Also check which countries are common in both the datasets, and which type of merge will you perform for the best results.
# Hint: Look on the 'how' parameter of merge function of pandas.

# Check common countries.
X_train_common = set(X_train['country']).intersection(set(countrydata['country_name']))
print(X_train_common)
X_test_common = set(X_test['country']).intersection(set(countrydata['country_name']))
print(X_test_common)

# Merge
X_train = pd.merge(X_train, countrydata, left_on='country', right_on='country_name', how='left')
X_test = pd.merge(X_test, countrydata, left_on='country', right_on='country_name', how='left')

X_train.head(1)

"""## 1.  EDA"""

print(X_train.dtypes)

"""Describe what you see above?"""

# Your answer:
# The above describes what data each variables have.
# 'object' refers to categorical variables while 'float64' refers to numerical variables.

"""Find out the number and percentage of missing values in the table per column"""

# Your code here:
missingvalues_X_train = X_train.isnull().mean()
missingvalues_X_train

"""Plot the frequency distribution / histogram of some of the numerical features that you think are important"""

# Your plotting code here:

# Import libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Create the histogram
variables = ['population_below_poverty_line', 'perceptions_of_corruption', 'healthy_life_expectancy']
fig, axes = plt.subplots(1, 3, figsize=(18, 6))
fig.suptitle('Histograms of Various Factors', fontsize=16)
for i, var in enumerate(variables):
    sns.histplot(data=X_train, x=var, kde=True, color='skyblue', edgecolor='black', ax=axes[i])
    axes[i].set_title(var.replace("_", " ").title())
    axes[i].set_xlabel('')

plt.tight_layout()
plt.show()

"""Plot the categorical variables and their distribution"""

# Your plotting code here:
cat_variables = ['country', 'region', 'country_name']
fig, axes = plt.subplots(1, 3, figsize=(18, 6))
fig.suptitle('Distribution of Categorical Variables', fontsize=16)
for i, var in enumerate(cat_variables):
    sns.countplot(data=X_train, x=var, ax=axes[i])
    axes[i].set_title(var.replace("_", " ").title())
    axes[i].set_xlabel('')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

"""Perform feature correlation analysis to identify relationships between variables. Use Pearson, Spearman, or Kendall correlation coefficients to analyze feature dependencies."""

# Your code here:

numerical_X_train = X_train.select_dtypes(include='float64')
numerical_X_train.corr(method='pearson')

"""Explore relationships between variables (bivariate, etc), correlation tables, and how they associate with the target variable."""

# Your plotting code(s) here:

# Calculate correlation.
correlation_matrix = numerical_X_train.corr()
plt.figure(figsize=(8, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap of Features')
plt.show()

# How it relates to target feature.
fig, axes = plt.subplots(4, 4, figsize=(16, 16))
fig.suptitle('Scatter Plots of Numerical Variables vs Target Variable', fontsize=16)

axes = axes.flatten()

# Create scatter plots
num_plots = min(len(numerical_X_train.columns), len(axes))
for i, column in enumerate(numerical_X_train.columns[:num_plots]):
    sns.scatterplot(x=numerical_X_train[column], y=y_train, ax=axes[i])
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Target')
    axes[i].set_title(f'{column} vs Target')

for j in range(num_plots, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.show()

"""Also, detect outliers using box plots, Z-score analysis, or the IQR method to identify potential data anomalies."""

# Your code here:

from scipy import stats

# Calculate Z-scores for each numerical X variable
z_scores = numerical_X_train.apply(stats.zscore)

# Identify potential outliers (Z-score > 3 or < -3)
outliers = (z_scores > 3) | (z_scores < -3)

# Print the number of outliers for each variable
print("Number of outliers per variable:")
print(outliers.sum())

"""Write what you observed and your General comments on what should be done:"""

# Your comments here:
# Five variables show one outlier each (gdp_per_capita, social_support,
# freedom_to_make_life_choices, generosity, perceptions_of_corruption).
# Using feature engineering we can create new features that bin the data or normalize the data.

"""## 2. Feature Engineering

Apply log transformations to normalize skewed data and improve model stability (If any).
"""

# Your code here:

columns_to_log = ['gdp_per_capita', 'population', 'gni']
for col in columns_to_log:
    X_train[f'{col}_log'] = np.log1p(X_train[col])

# Visualize change
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Original vs Log-transformed Variables')

for i, col in enumerate(columns_to_log):
    axes[0, i].hist(X_train[col])
    axes[0, i].set_title(f'Original {col}')
    axes[1, i].hist(X_train[f'{col}_log'])
    axes[1, i].set_title(f'Log-transformed {col}')

plt.tight_layout()
plt.show()

"""Create at least one interaction feature to capture relationship between existing variables, enhancing predictive power."""

# Your code here:
X_train['freedom_healthy'] = X_train['freedom_to_make_life_choices'] * X_train['healthy_life_expectancy']

"""## 3.   Preprocess data using Sklearn Column Transformer/ Write and Save Preprocessor function

"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Create the preprocessing pipelines for both numeric and categorical data.
numeric_features = X_train.select_dtypes(include=['float64'])
numeric_features=numeric_features.columns.tolist()

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')), ## Is this good enough?
    ('scaler', StandardScaler())]) # You will need to describe why this is being done in the next cell

categorical_features = ['region', 'country', 'country_name']

# Replacing missing values with Modal value and then one hot encoding.
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# Final preprocessor object set up with ColumnTransformer
preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),('cat', categorical_transformer, categorical_features)])

# Fit your preprocessor object
preprocess = preprocessor.fit(X_train)

"""Describe step-by-step what we are doing above, and why? You are free to change how values are imputed. What change did you make if any, and why?"""

## Your answer :
# For numerical features, the pipeline fills missing values with the median of each column
# with imputation and standardizes the features to have zero mean and unit variance with scaling.
# We changed the imputation strategy, because constant imputation with zero replaces all missing values with 0,
# assuming that "missing" and "zero" mean the same thing, which may not be the case.
# It specified categorical features, fills missing values with the most frequent value in each column with imputation,
# connverts categorical variables into binary columns with one-hot encoding.
# Then the piepline combines both numeric and categorical transformers into a single preprocessor,
# applies the appropriate transformer to each set of features, and fits the preprocessor to X_train.

# Write function to transform data with preprocessor

def preprocessor(data):
    data.drop(['country', 'region'], axis=1)
    preprocessed_data=preprocess.transform(data)
    return preprocessed_data

"""What are the differences between the "preprocessor" object, the "preprocess" object, the "preprocessor" function,  and the "preprocessed_data" that is returned finally?"""

## Your Answer :
# The "preprocessor" object defines the preprocessing steps.
# The "preprocess" object is the fitted version of the preprocessor.
# The "preprocessor" function is a wrapper that includes additional steps (dropping columns) and
# uses the fitted "preprocess" object. The "preprocessed_data" is the final, transformed data output.

# check shape of X data after preprocessing it using our new function
preprocessor(X_train).shape

"""## 4. Fit model on preprocessed data and save preprocessor function and model

"""

## Define a Random Forest Model here, fit it, and score it

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

model_1 = RandomForestClassifier(random_state=42)

# Fit model
model_1.fit(preprocessor(X_train), y_train)

# Apply same log transformations and interaction variable to X_test
columns_to_log = ['gdp_per_capita', 'population', 'gni']
for col in columns_to_log:
    X_test[f'{col}_log'] = np.log1p(X_test[col])

X_test['freedom_healthy'] = X_test['freedom_to_make_life_choices'] * X_test['healthy_life_expectancy']

# Score the model on training data
train_score = model_1.score(preprocessor(X_train), y_train)
print(f"Training Accuracy: {train_score:.4f}")

# Score the model on testing data
test_score = model_1.score(preprocessor(X_test), y_test)
print(f"Testing Accuracy: {test_score:.4f}")

# Your cell should have a score between 0-1 as output

"""## 5. Generate predictions from X_test data and compare it with true labels in Y_test

"""

#-- Generate predicted values (Model 1)
prediction_labels_1 = model_1.predict(preprocessor(X_test))

## Write code to show model performance by comparing prediction_labels with true labels
accuracy = accuracy_score(y_test, prediction_labels_1)
print(f"Accuracy: {accuracy:.4f}")

"""## 6. Repeat the process with different parameters to improve the accuracy

"""

# Train model 2 using same preprocessor (note that you could save a new preprocessor, but we will use the same one for this example).
from sklearn.ensemble import RandomForestClassifier

## Make a new model with changed parameters to improve the score
model_2 = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    max_features='sqrt',
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)

""" What changes did you make, what do the parameters you changed control, and why does it improve performance?"""

## Your answer :
# n_estimators=200 increases the number of trees to 200 to improve model stability and reduce variance.
# max_depth=10 limits the depth of each tree to prevent overfitting.
# max_features='sqrt' selects a subset of features at each split (square root of total features)
# to increase diversity among trees and improve overall generalization.
# min_samples_split=5 requires at least 5 samples to split a node, reducing overfitting.
# min_samples_leaf=2 ensures that leaf nodes have at least 2 samples,
# which prevents overly small splits that might capture noise.

#Evaluate Model 2:

## Write code to show model performance by comparing prediction_labels with true labels

# Fit Model 2
model_2.fit(preprocessor(X_train), y_train)

#-- Generate predicted y values (Model 2)
prediction_labels_2 = model_2.predict(preprocessor(X_test))

# Evaluate the model's performance using accuracy score
accuracy_2 = accuracy_score(y_test, prediction_labels_2)
print(f"Accuracy of Model 2: {accuracy_2:.4f}")

"""Do you think it is worth making more changes to the parameters? Should we keep trying random values and see what works better? What is an alternative to doing this manually?"""

## Your answer:
# The model performs better but trying random variables manually may not be efficient.
# We could use systematic methods to optimize hyperparameters with GridSearchCV.

# Submit a third model using GridSearchCV

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
import numpy as np

# Use np.arange to create a sequence of numbers for each parameter's space you think should be searched
param_grid = {
    'n_estimators': np.arange(100, 301, 50),
    'max_depth': np.arange(5, 16, 5),
    'min_samples_split': np.arange(2, 11, 3),
    'min_samples_leaf': np.arange(1, 5),
    'max_features': ['sqrt', 'log2']
}

# Read GridSearchCV docs and create an object with RandomForestClassifier as the model
model_3 = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                         param_grid=param_grid,
                         scoring='accuracy',
                         cv=5,
                         verbose=1)

# Fit the model using GridSearchCV
model_3.fit(preprocessor(X_train), y_train)

# Extract and print the best score and parameters
print("Best mean cross-validation score: {:.4f}".format(model_3.best_score_))
print("Best parameters: {}".format(model_3.best_params_))

#Submit Model 3:

#-- Generate predicted values
prediction_labels_3 = model_3.predict(preprocessor(X_test))

## Write code to show model performance by comparing prediction_labels with true labels
accuracy = accuracy_score(y_test, prediction_labels_3)
print(f"Accuracy: {accuracy:.4f}")

# Here are several classic ML architectures you can consider choosing from to experiment with next:
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import GradientBoostingClassifier

# Trying GradientBoostingClassifier
model_4 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Fit the model to the training data
history = model_4.fit(preprocessor(X_train), y_train)

#-- Generate predicted values
prediction_labels_4 = model_4.predict(preprocessor(X_test))

# Calculate accuracy
accuracy = accuracy_score(y_test, prediction_labels_4)
print(f"Accuracy: {accuracy:.4f}")

# Trying KNeighborsClassifier

model_5 = KNeighborsClassifier(n_neighbors=5)

# Fit the model to the training data
model_5.fit(preprocessor(X_train), y_train)

#-- Generate predicted values
prediction_labels_5 = model_5.predict(preprocessor(X_test))

# Calculate accuracy
accuracy = accuracy_score(y_test, prediction_labels_5)
print(f"Accuracy: {accuracy:.4f}")

# Trying SVC

model_6 = SVC(kernel='linear', C=0.1, random_state=42)

# Fit the model to the training data
model_6.fit(preprocessor(X_train), y_train)

#-- Generate predicted values
prediction_labels_6 = model_6.predict(preprocessor(X_test))

# Calculate accuracy
accuracy = accuracy_score(y_test, prediction_labels_6)
print(f"Accuracy: {accuracy:.4f}")

"""Describe what were the parameters you defined in GradientBoostingClassifier, and/or BaggingClassifier, and/or KNNs, and/or SVC? What worked and why?"""

## Your answer:

# With GradientBoostingClassifier, we built 100 sequential decision trees, kept each tree simple
# with maximum 3 levels deep, and addds each tree's predictions at a controlled rate of 0.1.
# This was intended to produce consistent results across multiple runs.

# With KNeighbors Classifier, we chose k = 5 to prevent the model from being
# too sensitive to noise and losing local patterns. However it is simple and not as powerful.

# With SVC, we chose linear kernel for a simple model.
# with a small C parameter for stronger regularization strength and to prevent overfitting.

# GridSearchCV performed the best of the 4.
#However, RandomForrestClassifier in model 2 performed better than the 4 subsequent models.

"""## 7. Basic Deep Learning"""

# Now experiment with deep learning models:
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation
from sklearn.preprocessing import LabelBinarizer

# Count features in input data
# The preprocessor function is likely changing the number of features
# We need to get the number of features *after* preprocessing
feature_count = preprocessor(X_train).shape[1] # Get feature count after preprocessing

num_classes = len(y_train.unique())

# Define a Neural Network Model with 5 layers 128->64->64->32->(?)
# Use Softmax activation in last layer. How many neurons should there be in the last layer?
keras_model = Sequential([
    Dense(128, input_dim=feature_count, activation='relu'), # Use the correct feature count
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(num_classes, activation='softmax') #Should be five classes
])

# Compile model
keras_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

# Convert y_train to one-hot encoding
lb = LabelBinarizer()
y_train_encoded = lb.fit_transform(y_train)

# Fitting the model to the Training set
history = keras_model.fit(preprocessor(X_train), y_train_encoded, ## Note that keras models require a one-hot-encoded y_train object
               batch_size = 20,
               epochs = 300, validation_split=0.25)

# Save history for plotting later
history_dict = history.history

"""Which activations did you use in the middle layers? Why was softmax used in the last layer?"""

## Your answer:
# We used ReLU to allow the network to learn more complex patterns and be more computationally efficient,
# as it simply returns the input for positive values and zero for negative values.
# Softmax is appropriate for multi-class classification problems where
# probabilities are across multiple possible output classes. It ensures that:
# All output probabilities range between 0 and 1 and all sum to 1 to form a probability distribution.
# The class with the highest probability is the model's prediction

"""Was it a good idea to train for 300 epochs? Should you train a bit more? Why or why not?"""

## Your answer:
# After 150 epochs, it appears that validation loss started to increase, hinting overfitting.
# We would recommend training on less epochs.

"""Why is loss='categorical_crossentropy' and optimizer='sgd'? Would you want to change something? Why / Why not?"""

## Your answer:
# 'categorical_crossentropy' can be used for multi-class classification problems where
# the classes are mutually exclusive and the target variable is one-hot encoded.
# 'sgd' is appropriate for convex optimization problems but may get stuck in local minima.
# We could try an adaptive optimizer instead of stochastic gradient descent.

"""Can you try getting the model's training history out and plotting the curves?"""

## Your code to plot training and validation curves in a single plot (Make changes in the model cell to be able to do this)

#plot loss and accuracy at each epoch
plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training', 'Validation'])

plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['Training', 'Validation'], loc='lower right')

#-- Generate predicted y values
y_pred = keras_model.predict(preprocessor(X_test))

# Note: Keras predict returns the predicted column index location for classification models
prediction_column_index = np.argmax(y_pred, axis=1)

# extract correct prediction labels
prediction_labels = [y_train.cat.categories[i] for i in prediction_column_index]

## Write code to show model performance by comparing prediction_labels with true labels
from sklearn.metrics import classification_report
print(classification_report(y_test, prediction_labels))

"""Implement regularization techniques such as Dropout and Batch Normalization to improve model generalization and observe change in performance. <br>
Note: Observe the training and testing loss and accuracy.
"""

# Your code here:

from keras.layers import Dropout, BatchNormalization

regularized_model = Sequential([
        # First layer
        Dense(128, input_dim=feature_count),
        BatchNormalization(),
        Activation('relu'),
        Dropout(0.3),  # 30% dropout

        # Second layer
        Dense(64),
        BatchNormalization(),
        Activation('relu'),
        Dropout(0.25),  # 25% dropout

        # Third layer
        Dense(64),
        BatchNormalization(),
        Activation('relu'),
        Dropout(0.25),  # 25% dropout

        # Fourth layer
        Dense(32),
        BatchNormalization(),
        Activation('relu'),
        Dropout(0.2),  # 20% dropout

        # Output layer
        Dense(num_classes, activation='softmax')
    ])

# Compile the model
regularized_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

# Fitting the model to the Training set
history = regularized_model.fit(preprocessor(X_train), y_train_encoded,
               batch_size = 20,
               epochs = 300, validation_split=0.25)

# Plot loss and accuracy at each epoch
plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training', 'Validation'])

plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['Training', 'Validation'], loc='lower right')

# Your comments about the change in performance:
# The regularized model shows lower training accuracy.
# The regularized model also should show more consistent improvement in validation accuracy.
# The original model's validation dipped initially but then starts increasing (U-curve).
# The regularized model's validation loss remains more consistent.

"""Experiment with different activation functions (ReLU, LeakyReLU, Tanh, Sigmoid) to observe their impact on model performance."""

# Your code here:

# LeakyReLU activation
from keras.layers import LeakyReLU

leaky_relu = Sequential([
        Dense(128, input_dim=feature_count),
        LeakyReLU(alpha=0.1),
        Dense(64),
        LeakyReLU(alpha=0.1),
        Dense(64),
        LeakyReLU(alpha=0.1),
        Dense(32),
        LeakyReLU(alpha=0.1),
        Dense(num_classes, activation='softmax')
    ])

# Compile the model
leaky_relu.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

# Fitting the model to the Training set
history = leaky_relu.fit(preprocessor(X_train), y_train_encoded,
               batch_size = 20,
               epochs = 300, validation_split=0.25)

#plot loss and accuracy at each epoch
plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training', 'Validation'])

plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['Training', 'Validation'], loc='lower right')

# Tanh activation
tanh_model = Sequential([
        Dense(128, input_dim=feature_count, activation='tanh'),
        Dense(64, activation='tanh'),
        Dense(64, activation='tanh'),
        Dense(32, activation='tanh'),
        Dense(num_classes, activation='softmax')
    ])

# Compile the model
tanh_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

# Fitting the model to the Training set
history = tanh_model.fit(preprocessor(X_train), y_train_encoded,
               batch_size = 20,
               epochs = 300, validation_split=0.25)

#plot loss and accuracy at each epoch
plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training', 'Validation'])

plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['Training', 'Validation'], loc='lower right')

# Sigmoid activation
sigmoid_model = Sequential([
        Dense(128, input_dim=feature_count, activation='sigmoid'),
        Dense(64, activation='sigmoid'),
        Dense(64, activation='sigmoid'),
        Dense(32, activation='sigmoid'),
        Dense(num_classes, activation='softmax')
    ])

# Compile the model
sigmoid_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

# Fitting the model to the Training set
history = sigmoid_model.fit(preprocessor(X_train), y_train_encoded,
               batch_size = 20,
               epochs = 300, validation_split=0.25)

#plot loss and accuracy at each epoch
plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training', 'Validation'])

plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['Training', 'Validation'], loc='lower right')

"""## 8. Explainability - SHAP Feature Importance

To better understand our model's predictions, we will use **SHAP (SHapley Additive exPlanations)** to analyze feature importance.  

### ðŸ”¹ How SHAP Works?
- SHAP assigns each feature a **contribution score** for every prediction.
- Uses **Shapley values** (from game theory) to fairly distribute importance across features.

We will now apply SHAP to visualize and interpret our modelâ€™s feature contributions.

"""

# Import libraries
import shap
from sklearn.impute import SimpleImputer

# SHAP Analysis:
# Create a wrapper function to handle NaN values during prediction:
def predict_wrapper(X):
    predictions = leaky_relu.predict(X)
    # Handle potential NaN values in predictions (replace with a default value, e.g., 0)
    predictions = np.nan_to_num(predictions)
    return predictions

# Initialize SHAP explainer using the wrapper function
explainer = shap.KernelExplainer(predict_wrapper, preprocessor(X_train))

# Compute SHAP values for X_test
shap_values = explainer.shap_values(preprocess.transform(X_test))

# Generate SHAP summary plot
shap.summary_plot(shap_values, preprocess.transform(X_test), feature_names=X_train.columns)

"""<h3> Experimentation"""

## You are encouraged to try more experimentation and any other models by adding more code cells to this notebook:

## You can also try to import any new dataset pertaining to countries, merge it, and see if it helps the predictions.
## If it does not, try to explain why it wasn't helpful by exploring variable relationships.

"""Deep learning models are often considered 'black boxes' due to their complexity. Explore methods such as SHAP (SHapley Additive exPlanations) to explain your model's predictions. After applying one of these methods, do you feel it provides a clear and sufficient explanation of how your model makes decisions? How easy or difficult is it to justify your model's predictions using these techniques?"""

## Your Code and Answer:

# Going back to keras model.
keras_model = Sequential([
    Dense(128, input_dim=feature_count, activation='relu'),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(5, activation='softmax')
])

# Compile model
keras_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

# Convert y_train to one-hot encoding
lb = LabelBinarizer()
y_train_encoded = lb.fit_transform(y_train)

# Fitting the model to the Training set
history = keras_model.fit(preprocessor(X_train), y_train_encoded,
               batch_size = 20,
               epochs = 300, validation_split=0.25)

from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.inspection import permutation_importance

# Permutation Feature Importance Analysis with NaN Handling:

class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, keras_model, preprocessor):
        self.keras_model = keras_model
        self.preprocessor = preprocessor

    def fit(self, X, y):
        return self

# Get predicted class labels
    def predict(self, X):
        preprocessed_X = self.preprocessor(X)  # Call the function directly
        predictions = self.keras_model.predict(preprocessed_X)
        predictions = np.nan_to_num(predictions, nan=0.0)
        return predictions.argmax(axis=1)  # Return class labels

# Create an instance of the wrapper class
wrapper = KerasClassifierWrapper(keras_model, preprocessor)

# Calculate baseline accuracy
y_test_labels = y_test.astype('category').cat.codes
baseline_accuracy = accuracy_score(y_test_labels, wrapper.predict(X_test))  # Use predict_wrapper

# Perform permutation importance using the wrapper instance
result = permutation_importance(
    estimator=wrapper,  # Use the wrapper instance
    X=X_test,  # Pass the original X_test
    y=y_test_labels,
    n_repeats=30,
    random_state=42,
    scoring='accuracy'
)

# Process and Visualize Results:

# 1. Get Feature Importances and Sort
importances = result.importances_mean
sorted_idx = importances.argsort()

# 2. Create a DataFrame for easier handling
df_importances = pd.DataFrame({
    "Feature": X_train.columns[sorted_idx],
    "Importance": importances[sorted_idx]
})

# 3. Plotting
fig, ax = plt.subplots()
ax.barh(df_importances["Feature"], df_importances["Importance"])
ax.set_title("Permutation Feature Importance")
ax.set_xlabel("Importance")
plt.show()

# 4. Display the DataFrame
print(df_importances)

"""SHAP interaction provides shows how each variable contributes to predictions for individual instances. However, running the model took a long time, showing that it is extremely resource-intensive. We can visualize variable attributions but the the conceptual reasoning the model uses remains vague. In combination with permutation importance, we were able to identify relevant features globally to compare with the detailed per-prediction explanation from SHAP."""